{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from gensim) (0.8)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from gensim) (1.5.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages (from gensim) (5.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from sqlalchemy import column\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket_name = 'deploy-sagemaker-conversation'\n",
    "\n",
    "\n",
    "s3_url = 's3://deploy-sagemaker-conversation/floop_data_15k.json'\n",
    "conn = boto3.client('s3')\n",
    "contents = conn.list_objects(Bucket = bucket_name)['Contents']\n",
    "\n",
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = conn.get_object(Bucket = bucket_name, Key = 'floop_data_15k.json')['Body'].read()\n",
    "data = pd.read_json(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15617, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns= [\"Field1\"]\n",
    "\n",
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "INFO - 18:52:12: Word2Vec lifecycle event {'params': 'Word2Vec(vocab=0, vector_size=100, alpha=0.03)', 'datetime': '2022-02-27T18:52:12.809042', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'created'}\n",
      "INFO - 18:52:12: collecting all words and their counts\n",
      "INFO - 18:52:12: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:52:12: PROGRESS: at sentence #10000, processed 80287 words, keeping 8478 word types\n",
      "INFO - 18:52:12: collected 11409 word types from a corpus of 130228 raw words and 15617 sentences\n",
      "INFO - 18:52:12: Creating a fresh vocabulary\n",
      "INFO - 18:52:12: Word2Vec lifecycle event {'msg': 'effective_min_count=20 retains 702 unique words (6.153037075992637%% of original 11409, drops 10707)', 'datetime': '2022-02-27T18:52:12.915848', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'prepare_vocab'}\n",
      "INFO - 18:52:12: Word2Vec lifecycle event {'msg': 'effective_min_count=20 leaves 104145 word corpus (79.97128113769696%% of original 130228, drops 26083)', 'datetime': '2022-02-27T18:52:12.916719', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'prepare_vocab'}\n",
      "INFO - 18:52:12: deleting the raw counts dictionary of 11409 items\n",
      "INFO - 18:52:12: sample=6e-05 downsamples 702 most-common words\n",
      "INFO - 18:52:12: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 20859.838993870264 word corpus (20.0%% of prior 104145)', 'datetime': '2022-02-27T18:52:12.927428', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'prepare_vocab'}\n",
      "INFO - 18:52:12: estimated required memory for 702 words and 100 dimensions: 912600 bytes\n",
      "INFO - 18:52:12: resetting layer weights\n",
      "INFO - 18:52:12: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-02-27T18:52:12.939510', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'build_vocab'}\n",
      "INFO - 18:52:12: Word2Vec lifecycle event {'msg': 'training model with 1 workers on 702 vocabulary and 100 features, using sg=0 hs=0 sample=6e-05 negative=20 window=2 shrink_windows=True', 'datetime': '2022-02-27T18:52:12.941106', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'train'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what', 'is', 'this', 'word']\n",
      "Time to build vocab: 0.0 mins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:52:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:13: EPOCH - 1 : training on 130228 raw words (20640 effective words) took 0.1s, 191995 effective words/s\n",
      "INFO - 18:52:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:13: EPOCH - 2 : training on 130228 raw words (20915 effective words) took 0.1s, 191762 effective words/s\n",
      "INFO - 18:52:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:13: EPOCH - 3 : training on 130228 raw words (20958 effective words) took 0.1s, 197459 effective words/s\n",
      "INFO - 18:52:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:13: EPOCH - 4 : training on 130228 raw words (20847 effective words) took 0.1s, 192512 effective words/s\n",
      "INFO - 18:52:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:13: EPOCH - 5 : training on 130228 raw words (20782 effective words) took 0.1s, 193009 effective words/s\n",
      "INFO - 18:52:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:13: EPOCH - 6 : training on 130228 raw words (20826 effective words) took 0.1s, 185213 effective words/s\n",
      "INFO - 18:52:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:13: EPOCH - 7 : training on 130228 raw words (20870 effective words) took 0.1s, 190719 effective words/s\n",
      "INFO - 18:52:13: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:13: EPOCH - 8 : training on 130228 raw words (21121 effective words) took 0.1s, 170025 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 9 : training on 130228 raw words (20834 effective words) took 0.1s, 162056 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 10 : training on 130228 raw words (20879 effective words) took 0.1s, 193756 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 11 : training on 130228 raw words (20998 effective words) took 0.1s, 193234 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 12 : training on 130228 raw words (20960 effective words) took 0.1s, 192595 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 13 : training on 130228 raw words (20913 effective words) took 0.1s, 195165 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 14 : training on 130228 raw words (20876 effective words) took 0.1s, 185439 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 15 : training on 130228 raw words (20948 effective words) took 0.1s, 188685 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 16 : training on 130228 raw words (20915 effective words) took 0.1s, 193848 effective words/s\n",
      "INFO - 18:52:14: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:14: EPOCH - 17 : training on 130228 raw words (20821 effective words) took 0.1s, 191586 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 18 : training on 130228 raw words (20806 effective words) took 0.1s, 191761 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 19 : training on 130228 raw words (20796 effective words) took 0.1s, 186889 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 20 : training on 130228 raw words (20756 effective words) took 0.1s, 193339 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 21 : training on 130228 raw words (20946 effective words) took 0.1s, 192848 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 22 : training on 130228 raw words (21036 effective words) took 0.1s, 192669 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 23 : training on 130228 raw words (20739 effective words) took 0.1s, 181099 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 24 : training on 130228 raw words (20764 effective words) took 0.1s, 193779 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 25 : training on 130228 raw words (20799 effective words) took 0.1s, 193700 effective words/s\n",
      "INFO - 18:52:15: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:15: EPOCH - 26 : training on 130228 raw words (20780 effective words) took 0.1s, 192545 effective words/s\n",
      "INFO - 18:52:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:16: EPOCH - 27 : training on 130228 raw words (20993 effective words) took 0.1s, 194533 effective words/s\n",
      "INFO - 18:52:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:16: EPOCH - 28 : training on 130228 raw words (20961 effective words) took 0.1s, 191620 effective words/s\n",
      "INFO - 18:52:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:16: EPOCH - 29 : training on 130228 raw words (20804 effective words) took 0.1s, 194022 effective words/s\n",
      "INFO - 18:52:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 18:52:16: EPOCH - 30 : training on 130228 raw words (21081 effective words) took 0.1s, 157462 effective words/s\n",
      "INFO - 18:52:16: Word2Vec lifecycle event {'msg': 'training on 3906840 raw words (626364 effective words) took 3.5s, 179333 effective words/s', 'datetime': '2022-02-27T18:52:16.434413', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'train'}\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:53: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "WARNING - 18:52:16: destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n",
      "INFO - 18:52:16: Word2Vec lifecycle event {'fname_or_handle': 'word2vec11.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-02-27T18:52:16.437397', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'saving'}\n",
      "INFO - 18:52:16: not storing attribute cum_table\n",
      "INFO - 18:52:16: saved word2vec11.model\n",
      "INFO - 18:52:16: loading Word2Vec object from word2vec11.model\n",
      "INFO - 18:52:16: loading wv recursively from word2vec11.model.wv.* with mmap=None\n",
      "INFO - 18:52:16: setting ignored attribute cum_table to None\n",
      "INFO - 18:52:16: Word2Vec lifecycle event {'fname': 'word2vec11.model', 'datetime': '2022-02-27T18:52:16.458958', 'gensim': '4.1.2', 'python': '3.6.13 | packaged by conda-forge | (default, Feb 19 2021, 05:36:01) \\n[GCC 9.3.0]', 'platform': 'Linux-4.14.252-131.483.amzn1.x86_64-x86_64-with-glibc2.9', 'event': 'loaded'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.06 mins\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#To remove special characters and punctuation from our dataset\n",
    "from string import punctuation\n",
    "\n",
    "punctuations = punctuation\n",
    "\n",
    "def solution(sentence):\n",
    "    for p in punctuations:\n",
    "        sentence = sentence.replace(p, '')\n",
    "    return sentence\n",
    "\n",
    "x = data[\"Field1\"].apply(solution)\n",
    "pattern = \"[^a-zA-Z0-9]\"\n",
    "x_cleaned = [re.sub(pattern,\" \",text) for text in x]\n",
    "\n",
    "x_lowered = [text.lower() for text in x_cleaned]\n",
    "x_lowered\n",
    "\n",
    "x_lowered[0]\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "x_tokenized = [nltk.word_tokenize(text) for text in x_lowered]\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "x_lemmatized = [[lemma.lemmatize(word) for word in text] for text in x_tokenized]\n",
    "\n",
    "print(x_lemmatized[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# For classification data whether good or bad.\n",
    "\n",
    "w2v_model = Word2Vec(min_count=20,window=2,sample=6e-5, alpha=0.03, min_alpha=0.0007, negative=20,workers= 1 )\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(x_lemmatized, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "t = time()\n",
    "\n",
    "w2v_model.train(x_lemmatized, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "\n",
    "w2v_model.init_sims(replace=True)\n",
    "\n",
    "w2v_model.save(\"word2vec11.model\")\n",
    "\n",
    "word_vectors = Word2Vec.load(\"word2vec11.model\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_clusters = 2\n",
    "kmeans = KMeans(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=\"s3://\" + bucket_name + \"/kmeans_cluster\",\n",
    "    k=num_clusters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:52:17: Same images used for training and inference. Defaulting to image scope: inference.\n",
      "WARNING - 18:52:17: Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n",
      "INFO - 18:52:17: Ignoring unnecessary instance type: None.\n",
      "INFO - 18:52:17: Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO - 18:52:17: Ignoring unnecessary instance type: None.\n",
      "INFO - 18:52:17: Same images used for training and inference. Defaulting to image scope: inference.\n",
      "WARNING - 18:52:17: Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n",
      "INFO - 18:52:17: Ignoring unnecessary instance type: None.\n",
      "INFO - 18:52:17: Creating training-job with name: kmeans-2022-02-27-18-52-17-581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-27 18:52:17 Starting - Starting the training job...\n",
      "2022-02-27 18:52:41 Starting - Launching requested ML instancesProfilerReport-1645987937: InProgress\n",
      "......\n",
      "2022-02-27 18:53:46 Starting - Preparing the instances for training.........\n",
      "2022-02-27 18:55:17 Downloading - Downloading input data...\n",
      "2022-02-27 18:55:48 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-input.json: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'true', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': ''}\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '100', 'k': '2', 'force_dense': 'True'}\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Final configuration: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'True', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': '', 'feature_dim': '100', 'k': '2'}\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 WARNING 139773837301568] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Using default worker.\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Create Store: local\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] nvidia-smi: took 0.032 seconds to run.\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Setting up with params: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'True', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': '', 'feature_dim': '100', 'k': '2'}\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] 'extra_center_factor' was set to 'auto', evaluated to 10.\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] number of center slices 1\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 WARNING 139773837301568] Batch size 5000 is bigger than the first batch data. Effective batch size used to initialize is 702\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1645988153.622811, \"EndTime\": 1645988153.6228483, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 702.0, \"count\": 1, \"min\": 702, \"max\": 702}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 702.0, \"count\": 1, \"min\": 702, \"max\": 702}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 702.0, \"count\": 1, \"min\": 702, \"max\": 702}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[2022-02-27 18:55:53.623] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 37, \"num_examples\": 1, \"num_bytes\": 300456}\u001b[0m\n",
      "\u001b[34m[2022-02-27 18:55:53.691] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 65, \"num_examples\": 1, \"num_bytes\": 300456}\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] processed a total of 702 examples\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1645988153.6231947, \"EndTime\": 1645988153.692106, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1404.0, \"count\": 1, \"min\": 1404, \"max\": 1404}, \"Total Batches Seen\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Max Records Seen Between Resets\": {\"sum\": 702.0, \"count\": 1, \"min\": 702, \"max\": 702}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 702.0, \"count\": 1, \"min\": 702, \"max\": 702}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] #throughput_metric: host=algo-1, train throughput=10163.973240360385 records/second\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 WARNING 139773837301568] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] shrinking 20 centers into 2\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #0. Current mean square distance 0.056726\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #1. Current mean square distance 0.082161\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #2. Current mean square distance 0.082161\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #3. Current mean square distance 0.065440\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #4. Current mean square distance 0.056726\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #5. Current mean square distance 0.056726\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #6. Current mean square distance 0.082161\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #7. Current mean square distance 0.065440\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #8. Current mean square distance 0.056726\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] local kmeans attempt #9. Current mean square distance 0.065361\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] finished shrinking process. Mean Square Distance = 0\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] #quality_metric: host=algo-1, train msd <loss>=0.056726180016994476\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] compute all data-center distances: inner product took: 36.7572%, (0.027628 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] gradient: cluster center took: 10.5409%, (0.007923 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] collect from kv store took: 9.8703%, (0.007419 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] splitting centers key-value pair took: 9.6274%, (0.007236 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] batch data loading with context took: 9.0872%, (0.006830 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] compute all data-center distances: point norm took: 8.5828%, (0.006451 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] gradient: one_hot took: 7.7949%, (0.005859 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] update state and report convergance took: 3.2960%, (0.002477 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] predict compute msd took: 3.2012%, (0.002406 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] update set-up time took: 0.5538%, (0.000416 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] gradient: cluster size  took: 0.3442%, (0.000259 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] compute all data-center distances: center norm took: 0.3099%, (0.000233 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] predict minus dist took: 0.0343%, (0.000026 secs)\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] TOTAL took: 0.07516336441040039\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1645988153.5846026, \"EndTime\": 1645988153.8177042, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 24.205446243286133, \"count\": 1, \"min\": 24.205446243286133, \"max\": 24.205446243286133}, \"epochs\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"update.time\": {\"sum\": 68.68147850036621, \"count\": 1, \"min\": 68.68147850036621, \"max\": 68.68147850036621}, \"_shrink.time\": {\"sum\": 121.06180191040039, \"count\": 1, \"min\": 121.06180191040039, \"max\": 121.06180191040039}, \"finalize.time\": {\"sum\": 123.02088737487793, \"count\": 1, \"min\": 123.02088737487793, \"max\": 123.02088737487793}, \"model.serialize.time\": {\"sum\": 2.1352767944335938, \"count\": 1, \"min\": 2.1352767944335938, \"max\": 2.1352767944335938}}}\u001b[0m\n",
      "\u001b[34m[02/27/2022 18:55:53 INFO 139773837301568] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1645988153.8177886, \"EndTime\": 1645988153.8181298, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 21.864652633666992, \"count\": 1, \"min\": 21.864652633666992, \"max\": 21.864652633666992}, \"totaltime\": {\"sum\": 337.51416206359863, \"count\": 1, \"min\": 337.51416206359863, \"max\": 337.51416206359863}}}\u001b[0m\n",
      "\n",
      "2022-02-27 18:56:02 Uploading - Uploading generated training model\n",
      "2022-02-27 18:56:02 Completed - Training job completed\n",
      "Training seconds: 45\n",
      "Billable seconds: 45\n"
     ]
    }
   ],
   "source": [
    "model = kmeans.fit(kmeans.record_set(word_vectors.vectors.astype('float32')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:56:31: Same images used for training and inference. Defaulting to image scope: inference.\n",
      "WARNING - 18:56:31: Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n",
      "INFO - 18:56:31: Ignoring unnecessary instance type: None.\n",
      "INFO - 18:56:31: Creating model with name: kmeans-2022-02-27-18-56-30-955\n",
      "INFO - 18:56:31: Creating endpoint-config with name kmeans-2022-02-27-18-56-30-955\n",
      "INFO - 18:56:31: Creating endpoint with name kmeans-2022-02-27-18-56-30-955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "kmeans_deployed = kmeans.deploy(initial_instance_count=1,instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.cluster_centers_\n",
    "\n",
    "def cast_vector(row):\n",
    "    return np.array(list(map(lambda x: x.astype('double'), row)))\n",
    "\n",
    "words = pd.DataFrame(word_vectors.index_to_key)\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors[f'{x}'])\n",
    "words['vectorsmean'] = words.vectors.apply(lambda x: x.mean())\n",
    "words['vectors_typed'] = words.vectors.apply(cast_vector)\n",
    "words['cluster'] = words.vectors_typed.apply(lambda x: kmeans_deployed.predict(x.astype('float32')))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])\n",
    "words['cluster_value'] = [1 if i==0 else -1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value\n",
    "\n",
    "words.head(10)\n",
    "\n",
    "u_labels = np.unique(words['cluster'])\n",
    "\n",
    "words['vectorsmean'] = words.vectors.apply(lambda x: x.mean())\n",
    "\n",
    "words['vectorsmean'][0]\n",
    "\n",
    "len(words[\"vectors\"][1])\n",
    "\n",
    "words.head(10)\n",
    "\n",
    "# FOr plotting \n",
    "\n",
    "colors = {1: 'black', -1: 'Red'}\n",
    "plt.scatter(words['sentiment_coeff'] , words['vectorsmean'] , c=words['cluster_value'].map(colors))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#os.remove(\"floop_data_15k.csv\")\n",
    "os.remove(\"word2vec11.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output = s3.Object(bucket_name,'kmeans_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_output = json.dumps({'numberOfClusters': num_clusters})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '01JB0MQXXASAJD17',\n",
       "  'HostId': '2vzd5CudO3l+SOcm1BIJvtVx0+5abKOqD5LUg3In6DFA6F7xq0Ghny2SvlVUXgbdHLDvS14V5AA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '2vzd5CudO3l+SOcm1BIJvtVx0+5abKOqD5LUg3In6DFA6F7xq0Ghny2SvlVUXgbdHLDvS14V5AA=',\n",
       "   'x-amz-request-id': '01JB0MQXXASAJD17',\n",
       "   'date': 'Sun, 27 Feb 2022 19:02:03 GMT',\n",
       "   'etag': '\"8d5913eb78d427852588f4d5a6744e27\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"8d5913eb78d427852588f4d5a6744e27\"'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_output.put(Body = bytes(json.dumps(json_output).encode('UTF-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
